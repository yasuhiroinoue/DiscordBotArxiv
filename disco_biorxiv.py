"""
Gemini Deep Research Tool with ArXiv & bioRxiv Integration - Discord Botç‰ˆ

ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€Gemini LLMã‚’ä½¿ç”¨ã—ã¦å­¦è¡“ç ”ç©¶ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
ArXivè«–æ–‡æ¤œç´¢ã€bioRxivè«–æ–‡æ¤œç´¢ã€ãŠã‚ˆã³Googleæ¤œç´¢ã‚’çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
Discordã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€çµæœã‚’Discordã«é€ä¿¡ã—ã¾ã™ã€‚
"""

import os
import time
import re
import requests
import io
import discord
from discord.ext import commands
import asyncio
import traceback
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")
google_cse_id = os.getenv("GOOGLE_CSE_ID")
DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")  # Discord Bot Token

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from bs4 import BeautifulSoup
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Callable

# LangChainé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_community import GoogleSearchAPIWrapper

# Discordã®è¨­å®š
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix=None, intents=intents)

# Discordç”¨å®šæ•°
MAX_DISCORD_LENGTH = 1900  # ä½™è£•ã‚’æŒãŸã›ã¦1900æ–‡å­—ã«è¨­å®š

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ç”¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
executor = ThreadPoolExecutor(max_workers=2)

# è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«
@dataclass
class Config:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    # APIè¨­å®š
    google_api_key: str = google_api_key
    google_cse_id: str = google_cse_id

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    llm_model: str = "gemini-2.0-flash"
    embedding_model: str = "models/embedding-001"
    
    # æ¤œç´¢è¨­å®š
    google_search_results: int = 10
    arxiv_max_papers: int = 100
    biorxiv_max_papers: int = 100
    arxiv_api_delay: int = 1
    keyword_limit: int = 20
    arxiv_research_max_papers: int = 6
    biorxiv_research_max_papers: int = 6
    
    # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†è¨­å®š
    chunk_size: int = 1000
    chunk_overlap: int = 200
    max_content_length: int = 10000

# è¨­å®šã®åˆæœŸåŒ–
config = Config()

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
os.environ["GOOGLE_API_KEY"] = config.google_api_key
os.environ["GOOGLE_CSE_ID"] = config.google_cse_id

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====

# å®‰å…¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
async def safe_send(channel, content):
    """
    Discordã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·åˆ¶é™ã‚’è€ƒæ…®ã—ã¦å®‰å…¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹
    """
    if not content:
        return
        
    if len(content) <= MAX_DISCORD_LENGTH:
        await channel.send(content)
    else:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²ã—ã¦é€ä¿¡
        await split_and_send_messages(channel, content)

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«é€ä¿¡
async def send_error(channel, error):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®‰å…¨ã«é€ä¿¡ã™ã‚‹
    é•·ã™ãã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯é©åˆ‡ã«çŸ­ç¸®ã™ã‚‹
    """
    error_str = str(error)
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
    if len(error_str) > MAX_DISCORD_LENGTH:
        error_str = error_str[:MAX_DISCORD_LENGTH - 100] + "...(çœç•¥)"
    
    await channel.send(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_str}")

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
def init_models_and_tools():
    """LLMãƒ¢ãƒ‡ãƒ«ã¨æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    # Gemini LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    llm = ChatGoogleGenerativeAI(model=config.llm_model)
    embeddings = GoogleGenerativeAIEmbeddings(model=config.embedding_model)
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²æ©Ÿèƒ½ã®åˆæœŸåŒ–
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap
    )
    
    # Googleæ¤œç´¢ã®åˆæœŸåŒ–
    google_search_available = False
    google_search_tool = None
    
    try:
        search = GoogleSearchAPIWrapper(k=config.google_search_results)
        google_search_tool = Tool(
            name="google_search",
            description="Search Google for recent results.",
            func=search.run
        )
        google_search_available = True
        print("Googleæ¤œç´¢ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"Googleæ¤œç´¢ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    return llm, embeddings, text_splitter, google_search_tool, google_search_available

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
llm, embeddings, text_splitter, google_search_tool, google_search_available = init_models_and_tools()

def check_arxiv_available() -> bool:
    """arxivãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒåˆ©ç”¨å¯èƒ½ã‹ã‚’ç¢ºèªã™ã‚‹"""
    try:
        import arxiv
        return True
    except ImportError:
        print("arxivãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("pip install arxiv ã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        return False

def format_papers_data(papers: List[Dict[str, Any]]) -> str:
    """è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—å½¢å¼ã«æ•´å½¢ã™ã‚‹ (ArXivã¨bioRxivä¸¡æ–¹ã«å¯¾å¿œ)"""
    if not papers:
        return ""
        
    papers_data = "## è«–æ–‡ãƒ‡ãƒ¼ã‚¿:\n"
    for i, paper in enumerate(papers):
        source = paper.get("source", "arxiv")  # ã‚½ãƒ¼ã‚¹ã‚’å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯arxiv
        papers_data += f"è«–æ–‡{i+1} [{source}]: {paper['title']}\n"
        papers_data += f"è‘—è€…: {paper['authors']}\n"
        papers_data += f"å…¬é–‹æ—¥: {paper['published']}\n"
        papers_data += f"URL: {paper['url']}\n"
        papers_data += f"è¦ç´„: {paper['summary']}\n"
        papers_data += f"è©³ç´°: {paper.get('detailed_summary', 'è¦ç´„ãªã—')}\n\n"
    
    return papers_data

def highlight_references(report: str) -> str:
    """å‚è€ƒæ–‡çŒ®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·èª¿è¡¨ç¤ºã—ã€URLã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã«å¤‰æ›ã™ã‚‹"""
    if "## å‚è€ƒæ–‡çŒ®" not in report:
        return report
        
    report_parts = report.split("## å‚è€ƒæ–‡çŒ®")
    references_section = "## å‚è€ƒæ–‡çŒ®" + report_parts[1]
    
    # å‚è€ƒæ–‡çŒ®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›´
    highlighted_references = references_section.replace(
        "## å‚è€ƒæ–‡çŒ®", 
        "## å‚è€ƒæ–‡çŒ®ï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸæƒ…å ±æºï¼‰"
    )
    
    # URLã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã«å¤‰æ›
    # URLãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹æ­£è¦è¡¨ç¾
    url_pattern = r'(https?://\S+)'
    
    # è¡Œã”ã¨ã«å‡¦ç†
    lines = highlighted_references.split('\n')
    for i, line in enumerate(lines):
        # URLã‚’æ¤œå‡º
        urls = re.findall(url_pattern, line)
        for url in urls:
            clean_url = url.rstrip('.,;:)')  # URLã®æœ«å°¾ã®è¨˜å·ã‚’é™¤å»
            # URLã‚’<URL>å½¢å¼ã«ç½®æ›ï¼ˆDiscordã§ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã«ãªã‚‹ï¼‰
            line = line.replace(url, f'<{clean_url}>')
        lines[i] = line
    
    highlighted_references = '\n'.join(lines)
    
    return report_parts[0] + highlighted_references

# ===== Discordç”¨ã®å…±é€šãƒ¡ã‚½ãƒƒãƒ‰ =====

async def split_and_send_messages(channel, text, max_length=MAX_DISCORD_LENGTH):
    """é•·æ–‡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é©åˆ‡ã«åˆ†å‰²ã—ã¦é€ä¿¡ã™ã‚‹"""
    if not text:
        await channel.send("ç”Ÿæˆã•ã‚ŒãŸçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    # éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é€ä¿¡
    if len(text) > MAX_DISCORD_LENGTH * 10:
        await channel.send("çµæœãŒéå¸¸ã«é•·ã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é€ä¿¡ã—ã¾ã™...")
        return await save_response_as_file(channel, text, "long_research_result.md")
    
    start = 0
    part_num = 1
    total_parts = (len(text) + max_length - 1) // max_length  # åˆ‡ã‚Šä¸Šã’é™¤ç®—
    
    while start < len(text):
        end = start + max_length
        
        # é©åˆ‡ãªåŒºåˆ‡ã‚Šä½ç½®ã‚’æ¢ã™ï¼ˆæ”¹è¡Œã‚„ç©ºç™½ãŒæœ›ã¾ã—ã„ï¼‰
        if end < len(text):
            # å¾Œæ–¹ã‹ã‚‰é©åˆ‡ãªåŒºåˆ‡ã‚Šä½ç½®ã‚’æ¢ã™
            pos = end
            while pos > start + max_length // 2:
                if text[pos] in ' \n\r\t':
                    end = pos
                    break
                pos -= 1
                
            # é©åˆ‡ãªåŒºåˆ‡ã‚Šä½ç½®ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯å¼·åˆ¶çš„ã«åŒºåˆ‡ã‚‹
            if pos == start + max_length // 2:
                end = start + max_length
                
        part_message = text[start:end].strip()
        
        try:
            if total_parts > 1:
                await channel.send(f"**ãƒ‘ãƒ¼ãƒˆ {part_num}/{total_parts}**\n{part_message}")
            else:
                await channel.send(part_message)
        except discord.HTTPException as e:
            # é€ä¿¡ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
            await channel.send(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é€ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚")
            return await save_response_as_file(channel, text, "error_research_result.md")
            
        start = end
        part_num += 1
        
        # å¤§é‡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹å ´åˆã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ã«å°‘ã—å¾…æ©Ÿ
        if total_parts > 3:
            await asyncio.sleep(1)

async def save_response_as_file(channel, response_text, filename=None):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¦é€ä¿¡ã™ã‚‹"""
    if filename is None:
        filename = "research_report.md"
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        file = discord.File(io.StringIO(response_text), filename=filename)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
        await channel.send(f"ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ:", file=file)
    except Exception as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«é€ä¿¡ã«å¤±æ•—ã—ãŸå ´åˆ
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)[:100]}..."
        await channel.send(error_msg)
        
        # å†…å®¹ã‚’è¦ç´„ã—ã¦é€ä¿¡
        if len(response_text) > 500:
            summary = response_text[:500] + "...(çœç•¥)"
            await channel.send(f"ãƒ¬ãƒãƒ¼ãƒˆã®ä¸€éƒ¨:\n```\n{summary}\n```")

# ===== æ¤œç´¢æ©Ÿèƒ½ =====

def web_search(query: str) -> str:
    """Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—çµæœã‚’è¿”ã™"""
    print(f"\n---Googleæ¤œç´¢ã®å®Ÿè¡Œ: '{query}'---")
    
    if not google_search_available:
        print("Googleæ¤œç´¢ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã“ã®éƒ¨åˆ†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return "Googleæ¤œç´¢ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
    
    try:
        results = google_search_tool.run(query)
        print("æ¤œç´¢çµæœã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        return results
    except Exception as e:
        print(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def arxiv_research(query: str, max_papers: int = None) -> Dict[str, Any]:
    """ArXivã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ã—ã¦å‡¦ç†ã™ã‚‹"""
    if max_papers is None:
        max_papers = config.arxiv_max_papers
        
    print(f"\n------ ArXivã§ã€Œ{query}ã€ã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢ä¸­... ------")
    
    if not check_arxiv_available():
        return {"status": "error", "message": "arxivãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“", "papers": []}
    
    try:
        import arxiv
        
        # arxivã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç›´æ¥ä½¿ç”¨
        client = arxiv.Client()
        search = arxiv.Search(
            query=query,
            max_results=max_papers,
            sort_by=arxiv.SortCriterion.Relevance
        )
        
        papers = list(client.results(search))
        print(f"ArXivæ¤œç´¢çµæœ: {len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        
        if not papers:
            print("ArXivæ¤œç´¢: è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return {"status": "error", "message": "è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "papers": []}
        
        # æ¤œç´¢çµæœã‚’æ•´å½¢
        paper_results = []
        for i, paper in enumerate(papers):
            paper_url = f"https://arxiv.org/abs/{paper.get_short_id()}"
            print(f"è«–æ–‡{i+1}: {paper.title} - {paper_url}")
            
            # æœ¬æ–‡ã‚’å–å¾—
            content = f"ã‚¿ã‚¤ãƒˆãƒ«: {paper.title}\nè¦ç´„: {paper.summary}"
            
            paper_info = {
                "title": paper.title,
                "authors": ", ".join(author.name for author in paper.authors),
                "published": paper.published.strftime("%Y-%m-%d"),
                "summary": paper.summary,
                "url": paper_url,
                "content": content,
                "source": "arxiv"  # ã‚½ãƒ¼ã‚¹è­˜åˆ¥ç”¨
            }
            paper_results.append(paper_info)
            if i < len(papers) - 1:
                time.sleep(config.arxiv_api_delay)  # APIåˆ¶é™ã«é…æ…®
        
        return {
            "status": "success", 
            "papers": paper_results,
            "count": len(paper_results)
        }
        
    except Exception as e:
        print(f"ArXivæ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return {"status": "error", "message": f"ArXivæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "papers": []}

def get_biorxiv_abstract(doi: str) -> str:
    """bioRxiv APIã‚’ä½¿ç”¨ã—ã¦è«–æ–‡ã®è¦ç´„ã‚’å–å¾—ã™ã‚‹"""
    try:
        # DOIã‹ã‚‰bioRxiv APIã‚’ä½¿ç”¨
        api_url = f"https://api.biorxiv.org/details/biorxiv/{doi}"
        response = requests.get(api_url)
        
        if response.status_code == 200:
            data = response.json()
            collection = data.get("collection", [])
            if collection:
                abstract = collection[0].get("abstract", "")
                if abstract:
                    return abstract
        
        # APIã§å–å¾—ã§ããªã„å ´åˆã¯Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§è©¦è¡Œ
        paper_url = f"https://www.biorxiv.org/content/{doi}v1"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(paper_url, headers=headers)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            abstract_elem = soup.find('div', class_='abstract')
            if abstract_elem:
                return abstract_elem.text.strip().replace('Abstract', '', 1).strip()
    
    except Exception as e:
        print(f"è¦ç´„å–å¾—ã‚¨ãƒ©ãƒ¼ (DOI {doi}): {e}")
    
    return "è¦ç´„ãªã—"

def fallback_to_web_search(query: str, max_papers: int) -> Dict[str, Any]:
    """PubMedæ¤œç´¢ãŒå¤±æ•—ã—ãŸå ´åˆã®Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    print(f"bioRxiv Webã‚µã‚¤ãƒˆç›´æ¥æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
    
    try:
        # bioRxivæ¤œç´¢ãƒšãƒ¼ã‚¸
        search_url = f"https://www.biorxiv.org/search/{query}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(search_url, headers=headers)
        
        if response.status_code != 200:
            print(f"bioRxiv Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            return {"status": "error", "message": f"bioRxiv Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {response.status_code}", "papers": []}
        
        # HTMLã®è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        papers = soup.find_all('li', class_='search-result')
        
        if not papers:
            print("bioRxivæ¤œç´¢: è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return {"status": "error", "message": "è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "papers": []}
        
        # æ¤œç´¢çµæœã‚’æ•´å½¢
        paper_results = []
        count = 0
        
        for paper in papers:
            if count >= max_papers:
                break
                
            title_elem = paper.find('span', class_='highwire-cite-title')
            if not title_elem:
                continue
                
            title = title_elem.text.strip()
            
            # è‘—è€…æƒ…å ±
            authors_elem = paper.find('span', class_='highwire-citation-authors')
            authors = authors_elem.text.strip() if authors_elem else 'è‘—è€…ä¸æ˜'
            
            # URLæƒ…å ±
            link_elem = title_elem.find('a')
            paper_url = f"https://www.biorxiv.org{link_elem['href']}" if link_elem and 'href' in link_elem.attrs else ""
            
            # DOIæƒ…å ±ã‚’æŠ½å‡º
            doi = ""
            if paper_url:
                doi_match = re.search(r'10\.1101/(.+?)($|/)', paper_url)
                if doi_match:
                    doi = "10.1101/" + doi_match.group(1)
            
            # è¦ç´„ï¼ˆã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆï¼‰ã®å–å¾—
            abstract = ""
            if paper_url:
                try:
                    paper_response = requests.get(paper_url, headers=headers)
                    if paper_response.status_code == 200:
                        paper_soup = BeautifulSoup(paper_response.text, 'html.parser')
                        abstract_elem = paper_soup.find('div', class_='abstract')
                        if abstract_elem:
                            abstract = abstract_elem.text.strip().replace('Abstract', '', 1).strip()
                except Exception as e:
                    print(f"è«–æ–‡è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ—¥ä»˜æƒ…å ±
            date_elem = paper.find('span', class_='highwire-cite-metadata-date')
            date = date_elem.text.strip() if date_elem else 'æ—¥ä»˜ä¸æ˜'
            
            print(f"è«–æ–‡{count+1}: {title} - {paper_url}")
            
            # è«–æ–‡æƒ…å ±ã‚’æ•´å½¢
            content = f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\nè¦ç´„: {abstract}"
            
            paper_info = {
                "title": title,
                "authors": authors,
                "published": date,
                "summary": abstract,
                "url": paper_url,
                "doi": doi,
                "content": content,
                "source": "biorxiv"
            }
            paper_results.append(paper_info)
            count += 1
            
            if count < max_papers:
                time.sleep(config.arxiv_api_delay)  # éè² è·é˜²æ­¢
        
        return {
            "status": "success", 
            "papers": paper_results,
            "count": len(paper_results)
        }
        
    except Exception as e:
        print(f"bioRxiv Webæ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return {"status": "error", "message": f"bioRxiv Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "papers": []}

def biorxiv_search(query: str, max_papers: int = None) -> Dict[str, Any]:
    """PubMed APIã‚’ä½¿ç”¨ã—ã¦bioRxivè«–æ–‡ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã™ã‚‹"""
    if max_papers is None:
        max_papers = config.biorxiv_max_papers
        
    print(f"\n------ bioRxivã§ã€Œ{query}ã€ã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢ä¸­ (PubMed APIçµŒç”±)... ------")
    
    try:
        # bioRxivã«çµã£ãŸPubMedæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        pubmed_query = f'{query} AND "bioRxiv"[Journal]'
        
        # PubMedæ¤œç´¢ã‚’å®Ÿè¡Œ
        search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        search_params = {
            "db": "pubmed",
            "term": pubmed_query,
            "retmode": "json",
            "retmax": max_papers
        }
        search_response = requests.get(search_url, params=search_params)
        
        if search_response.status_code != 200:
            print(f"PubMedæ¤œç´¢ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {search_response.status_code}")
            return fallback_to_web_search(query, max_papers)
        
        search_results = search_response.json()
        id_list = search_results.get("esearchresult", {}).get("idlist", [])
        
        if not id_list:
            print("PubMedæ¤œç´¢: è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            return fallback_to_web_search(query, max_papers)
            
        print(f"PubMedæ¤œç´¢çµæœ: {len(id_list)}ä»¶ã®bioRxivè«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        
        # è«–æ–‡è©³ç´°æƒ…å ±ã‚’å–å¾—
        details_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
        details_params = {
            "db": "pubmed",
            "id": ",".join(id_list),
            "retmode": "json"
        }
        details_response = requests.get(details_url, params=details_params)
        
        if details_response.status_code != 200:
            print(f"PubMedè©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {details_response.status_code}")
            return fallback_to_web_search(query, max_papers)
            
        details_data = details_response.json()
        
        # çµæœã‚’æ•´å½¢
        paper_results = []
        for pmid in id_list:
            try:
                paper_data = details_data["result"][pmid]
                
                # è«–æ–‡æƒ…å ±ã‚’æŠ½å‡º
                title = paper_data.get("title", "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
                authors_list = paper_data.get("authors", [])
                authors = ", ".join([author.get("name", "") for author in authors_list if author.get("name")])
                
                # DOIã®æŠ½å‡º
                article_ids = paper_data.get("articleids", [])
                doi = ""
                for id_obj in article_ids:
                    if id_obj.get("idtype") == "doi":
                        doi = id_obj.get("value", "")
                        break
                
                # bioRxiv DOIã®å½¢å¼ãƒã‚§ãƒƒã‚¯
                if doi and doi.startswith("10.1101/"):
                    # DOIã‹ã‚‰bioRxiv URLã‚’æ§‹ç¯‰
                    paper_url = f"https://doi.org/{doi}"
                    
                    # è¦ç´„ã¯PubMedã‹ã‚‰ç›´æ¥å–å¾—ã§ããªã„ã®ã§ã€bioRxiv APIã§å–å¾—ã‚’è©¦ã¿ã‚‹
                    abstract = get_biorxiv_abstract(doi)
                    
                    published_date = paper_data.get("pubdate", "æ—¥ä»˜ä¸æ˜")
                    
                    print(f"è«–æ–‡: {title} - {paper_url}")
                    
                    # è«–æ–‡æƒ…å ±ã‚’æ•´å½¢
                    content = f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\nè¦ç´„: {abstract}"
                    
                    paper_info = {
                        "title": title,
                        "authors": authors,
                        "published": published_date,
                        "summary": abstract,
                        "url": paper_url,
                        "doi": doi,
                        "content": content,
                        "source": "biorxiv"
                    }
                    paper_results.append(paper_info)
            except Exception as e:
                print(f"è«–æ–‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ (PMID {pmid}): {e}")
        
        if not paper_results:
            print("æœ‰åŠ¹ãªbioRxivè«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            return fallback_to_web_search(query, max_papers)
            
        return {
            "status": "success", 
            "papers": paper_results,
            "count": len(paper_results)
        }
        
    except Exception as e:
        print(f"PubMedæ¤œç´¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return fallback_to_web_search(query, max_papers)

def optimized_arxiv_search(topic: str, max_papers: int = None) -> Dict[str, Any]:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ArXivæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹"""
    if max_papers is None:
        max_papers = config.arxiv_max_papers
        
    if not check_arxiv_available():
        return {"status": "error", "message": "arxivãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“", "papers": []}
    
    try:
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–
        query_optimization_prompt = PromptTemplate(
            input_variables=["topic"],
            template="""
            ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã«å¯¾ã™ã‚‹ArXivæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚
            ã§ãã‚‹ã ã‘é–¢é€£æ€§ã®é«˜ã„è«–æ–‡ãŒæ¤œç´¢ã§ãã‚‹ã‚ˆã†ã€é©åˆ‡ãªæ¤œç´¢æ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
            è‹±èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚æ¤œç´¢ã‚¯ã‚¨ãƒªã ã‘ã‚’å‡ºåŠ›ã—ã€ä½™åˆ†ãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
            
            ç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}
            """
        )
        
        # Runnable APIã‚’ä½¿ç”¨
        query_chain = query_optimization_prompt | llm | StrOutputParser()
        optimized_query = query_chain.invoke({"topic": topic}).strip()
        
        print(f"æœ€é©åŒ–ã•ã‚ŒãŸArXivã‚¯ã‚¨ãƒª: {optimized_query}")
        
        # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§æ¤œç´¢
        result = arxiv_research(optimized_query, max_papers=max_papers)
        
        # çµæœãŒãªã„å ´åˆã¯åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å†è©¦è¡Œ
        if result["status"] == "error" or len(result.get("papers", [])) == 0:
            print("æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§çµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚åŸºæœ¬ã‚¯ã‚¨ãƒªã§å†è©¦è¡Œã—ã¾ã™ã€‚")
            # è‹±èªã«ç¿»è¨³ã—ã¦å†è©¦è¡Œ
            translation_prompt = PromptTemplate(
                input_variables=["topic"],
                template="ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ç¿»è¨³ã ã‘ã‚’å‡ºåŠ›ã—ã€ä½™åˆ†ãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚\n\nç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}"
            )
            translation_chain = translation_prompt | llm | StrOutputParser()
            english_topic = translation_chain.invoke({"topic": topic}).strip()
            
            print(f"è‹±èªãƒˆãƒ”ãƒƒã‚¯ã«å¤‰æ›: '{english_topic}'")
            result = arxiv_research(english_topic, max_papers=max_papers)
        
        return result
    
    except Exception as e:
        print(f"æ¤œç´¢æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return {"status": "error", "message": f"æ¤œç´¢æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}", "papers": []}

def optimized_biorxiv_search(topic: str, max_papers: int = None) -> Dict[str, Any]:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦bioRxivæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹"""
    if max_papers is None:
        max_papers = config.biorxiv_max_papers
    
    try:
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–
        query_optimization_prompt = PromptTemplate(
            input_variables=["topic"],
            template="""
            ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã«å¯¾ã™ã‚‹PubMedæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚
            ç”Ÿç‰©å­¦ãƒ»åŒ»å­¦åˆ†é‡ã®ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆã‚µãƒ¼ãƒãƒ¼bioRxivã§ã®æ¤œç´¢ã«é©ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã—ã¦ãã ã•ã„ã€‚
            è‹±èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚æ¤œç´¢ã‚¯ã‚¨ãƒªã ã‘ã‚’å‡ºåŠ›ã—ã€ä½™åˆ†ãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
            
            PubMedã¨bioRxivæ¤œç´¢ç”¨ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã¾ã™ã€‚
            
            ç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}
            """
        )
        
        # Runnable APIã‚’ä½¿ç”¨
        query_chain = query_optimization_prompt | llm | StrOutputParser()
        optimized_query = query_chain.invoke({"topic": topic}).strip()
        
        print(f"æœ€é©åŒ–ã•ã‚ŒãŸbioRxivã‚¯ã‚¨ãƒª: {optimized_query}")
        
        # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§æ¤œç´¢
        result = biorxiv_search(optimized_query, max_papers=max_papers)
        
        # çµæœãŒãªã„å ´åˆã¯åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å†è©¦è¡Œ
        if result["status"] == "error" or len(result.get("papers", [])) == 0:
            print("æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§çµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚åŸºæœ¬ã‚¯ã‚¨ãƒªã§å†è©¦è¡Œã—ã¾ã™ã€‚")
            # è‹±èªã«ç¿»è¨³ã—ã¦å†è©¦è¡Œ
            translation_prompt = PromptTemplate(
                input_variables=["topic"],
                template="ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ç”Ÿç‰©å­¦ç”¨èªãŒã‚ã‚‹å ´åˆã¯ã€é©åˆ‡ãªè‹±èªã®å°‚é–€ç”¨èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ç¿»è¨³ã ã‘ã‚’å‡ºåŠ›ã—ã€ä½™åˆ†ãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚\n\nç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}"
            )
            translation_chain = translation_prompt | llm | StrOutputParser()
            english_topic = translation_chain.invoke({"topic": topic}).strip()
            
            print(f"è‹±èªãƒˆãƒ”ãƒƒã‚¯ã«å¤‰æ›: '{english_topic}'")
            result = biorxiv_search(english_topic, max_papers=max_papers)
        
        return result
    
    except Exception as e:
        print(f"æ¤œç´¢æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return {"status": "error", "message": f"æ¤œç´¢æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}", "papers": []}

# ===== ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†æ©Ÿèƒ½ =====

def summarize_paper(paper_content: str) -> str:
    """è«–æ–‡ã®å†…å®¹ã‚’è¦ç´„ã™ã‚‹"""
    if not paper_content or len(paper_content) < 100:
        return "ååˆ†ãªå†…å®¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
    
    # è¦ç´„ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt_template = PromptTemplate(
        template="""
        ä»¥ä¸‹ã¯å­¦è¡“è«–æ–‡ã®å†…å®¹ã§ã™ã€‚ã“ã®è«–æ–‡ã®ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã€æ–¹æ³•è«–ã€çµæœã€ãŠã‚ˆã³æ„ç¾©ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        å°‚é–€ç”¨èªã¯ä¿æŒã—ã¤ã¤ã€ç†è§£ã—ã‚„ã™ã„æ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        
        è«–æ–‡å†…å®¹:
        {text}
        
        è¦ç´„:
        """,
        input_variables=["text"]
    )
    
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²ã—ã¦å‡¦ç†
        if len(paper_content) > config.max_content_length:
            paper_content = paper_content[:config.max_content_length] + "..."
        
        # Runnable APIã‚’ä½¿ç”¨
        summary_chain = prompt_template | llm | StrOutputParser()
        summary = summary_chain.invoke({"text": paper_content})
        return summary
    except Exception as e:
        print(f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"

def analyze_paper(paper_url: str) -> str:
    """è«–æ–‡ã®è©³ç´°åˆ†æã‚’è¡Œã†"""
    try:
        print(f"\nè«–æ–‡åˆ†æã‚’é–‹å§‹: {paper_url}")
        
        # ArXivè«–æ–‡ã‹bioRxivè«–æ–‡ã‹ã‚’åˆ¤æ–­
        if "arxiv" in paper_url.lower():
            return analyze_arxiv_paper(paper_url)
        elif "biorxiv" in paper_url.lower() or "doi.org" in paper_url.lower():
            return analyze_biorxiv_paper(paper_url)
        else:
            # URLã‹ã‚‰ã©ã®ã‚¿ã‚¤ãƒ—ã‹åˆ¤æ–­ã§ããªã„å ´åˆ
            if paper_url.startswith("10.1101/"):
                # DOIå½¢å¼ã§bioRxivã¨åˆ¤æ–­
                return analyze_biorxiv_paper(paper_url)
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ArXivã¨ã—ã¦æ‰±ã†
                return analyze_arxiv_paper(paper_url)
    
    except Exception as e:
        print(f"è«–æ–‡åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return f"è«–æ–‡åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

def analyze_arxiv_paper(paper_url: str) -> str:
    """ArXivè«–æ–‡ã®è©³ç´°åˆ†æã‚’è¡Œã†"""
    try:
        print(f"\nArXivè«–æ–‡åˆ†æã‚’é–‹å§‹: {paper_url}")
        paper_id = paper_url.split("/")[-1]
        
        if not check_arxiv_available():
            return "arxivãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install arxiv ã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚"
        
        import arxiv
        
        # ç‰¹å®šã®è«–æ–‡IDã§æ¤œç´¢
        client = arxiv.Client()
        search = arxiv.Search(
            id_list=[paper_id],
            max_results=1
        )
        
        papers = list(client.results(search))
        
        if not papers:
            return "è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        paper = papers[0]
        paper_url = f"https://arxiv.org/abs/{paper.get_short_id()}"
        print(f"è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ: {paper.title}")
        
        # è«–æ–‡ã®è©³ç´°åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        analysis_prompt = PromptTemplate(
            input_variables=["title", "authors", "abstract", "url"],
            template="""
            ä»¥ä¸‹ã®è«–æ–‡ã«ã¤ã„ã¦è©³ç´°ãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„:
            
            ã‚¿ã‚¤ãƒˆãƒ«: {title}
            è‘—è€…: {authors}
            ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ: {abstract}
            è«–æ–‡URL: {url}
            
            ä»¥ä¸‹ã®æ§‹é€ ã§åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„:
            
            # è«–æ–‡ã€Œ{title}ã€ã®åˆ†æ
            
            ## ç ”ç©¶ã®ä¸»ãªç›®çš„
            [è«–æ–‡ã®ç›®çš„ã€èƒŒæ™¯ã€ç ”ç©¶èª²é¡Œã®èª¬æ˜]
            
            ## ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ–¹æ³•è«–
            [è‘—è€…ãŒæ¡ç”¨ã—ãŸç ”ç©¶æ‰‹æ³•ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã©ã®èª¬æ˜]
            
            ## ä¸»è¦ãªçµæœã¨ç™ºè¦‹
            [è«–æ–‡ã®é‡è¦ãªçµæœã€ç™ºè¦‹ã€æ•°å€¤ãªã©ã®è¦ç´„]
            
            ## ç ”ç©¶ã®æ„ç¾©ã¨å½±éŸ¿
            [æœ¬ç ”ç©¶ã®å­¦è¡“çš„ãƒ»å®Ÿç”¨çš„ãªé‡è¦æ€§ã®åˆ†æ]
            
            ## åˆ¶é™äº‹é …ã¨ä»Šå¾Œã®ç ”ç©¶æ–¹å‘
            [è‘—è€…ãŒèªè­˜ã—ã¦ã„ã‚‹åˆ¶é™äº‹é …ã¨å°†æ¥ã®ç ”ç©¶æ©Ÿä¼š]
            
            ## å‚è€ƒæ–‡çŒ®æƒ…å ±
            è«–æ–‡æƒ…å ±: {title} - {authors} - {url}
            """
        )
        
        # Runnable APIã‚’ä½¿ç”¨
        analysis_chain = analysis_prompt | llm | StrOutputParser()
        analysis = analysis_chain.invoke({
            "title": paper.title,
            "authors": ", ".join(author.name for author in paper.authors),
            "abstract": paper.summary,
            "url": paper_url
        })
        
        return analysis
    
    except Exception as e:
        print(f"ArXivè«–æ–‡åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return f"ArXivè«–æ–‡åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

def analyze_biorxiv_paper(paper_doi: str) -> str:
    """bioRxivè«–æ–‡ã®è©³ç´°åˆ†æã‚’è¡Œã†"""
    try:
        print(f"\nbioRxivè«–æ–‡åˆ†æã‚’é–‹å§‹: {paper_doi}")
        
        # DOIã®æŠ½å‡ºï¼ˆURLã‹ã‚‰ï¼‰
        if "doi.org/" in paper_doi:
            paper_doi = paper_doi.split("doi.org/")[-1]
        
        # bioRxiv DOIã®å½¢å¼ï¼ˆ10.1101/...ï¼‰ç¢ºèª
        if not paper_doi.startswith("10.1101/"):
            paper_doi = "10.1101/" + paper_doi
        
        # DOIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—
        url = f"https://api.biorxiv.org/details/biorxiv/{paper_doi}"
        response = requests.get(url)
        
        if response.status_code != 200:
            return f"bioRxiv API ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}"
        
        data = response.json()
        collection = data.get("collection", [])
        
        if not collection:
            # APIå–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦ã¿ã‚‹
            print("APIçµŒç”±ã§ã®è«–æ–‡æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦ã¿ã¾ã™ã€‚")
            try:
                paper_url = f"https://doi.org/{paper_doi}"
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                response = requests.get(paper_url, headers=headers)
                
                if response.status_code != 200:
                    return "è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title_elem = soup.find('meta', {'name': 'citation_title'})
                title = title_elem['content'] if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                
                # è‘—è€…å–å¾—
                author_elems = soup.find_all('meta', {'name': 'citation_author'})
                authors = ", ".join([elem['content'] for elem in author_elems]) if author_elems else "è‘—è€…ä¸æ˜"
                
                # è¦ç´„å–å¾—
                abstract_elem = soup.find('div', class_='abstract')
                abstract = abstract_elem.text.strip().replace('Abstract', '', 1).strip() if abstract_elem else "è¦ç´„ãªã—"
                
            except Exception as e:
                print(f"Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                return "è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        else:
            paper = collection[0]
            title = paper.get('title', 'ç„¡é¡Œ')
            authors = paper.get('authors', 'è‘—è€…ä¸æ˜')
            abstract = paper.get('abstract', 'è¦ç´„ãªã—')
            paper_url = f"https://doi.org/{paper_doi}"
        
        print(f"è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ: {title}")
        
        # è«–æ–‡ã®è©³ç´°åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        analysis_prompt = PromptTemplate(
            input_variables=["title", "authors", "abstract", "url"],
            template="""
            ä»¥ä¸‹ã®bioRxivè«–æ–‡ã«ã¤ã„ã¦è©³ç´°ãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„:
            
            ã‚¿ã‚¤ãƒˆãƒ«: {title}
            è‘—è€…: {authors}
            ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ: {abstract}
            è«–æ–‡URL: {url}
            
            ä»¥ä¸‹ã®æ§‹é€ ã§åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„:
            
            # è«–æ–‡ã€Œ{title}ã€ã®åˆ†æ
            
            ## ç ”ç©¶ã®ä¸»ãªç›®çš„
            [è«–æ–‡ã®ç›®çš„ã€èƒŒæ™¯ã€ç ”ç©¶èª²é¡Œã®èª¬æ˜]
            
            ## ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ–¹æ³•è«–
            [è‘—è€…ãŒæ¡ç”¨ã—ãŸç ”ç©¶æ‰‹æ³•ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã©ã®èª¬æ˜]
            
            ## ä¸»è¦ãªçµæœã¨ç™ºè¦‹
            [è«–æ–‡ã®é‡è¦ãªçµæœã€ç™ºè¦‹ã€æ•°å€¤ãªã©ã®è¦ç´„]
            
            ## ç ”ç©¶ã®æ„ç¾©ã¨å½±éŸ¿
            [æœ¬ç ”ç©¶ã®å­¦è¡“çš„ãƒ»å®Ÿç”¨çš„ãªé‡è¦æ€§ã®åˆ†æ]
            
            ## åˆ¶é™äº‹é …ã¨ä»Šå¾Œã®ç ”ç©¶æ–¹å‘
            [è‘—è€…ãŒèªè­˜ã—ã¦ã„ã‚‹åˆ¶é™äº‹é …ã¨å°†æ¥ã®ç ”ç©¶æ©Ÿä¼š]
            
            ## å‚è€ƒæ–‡çŒ®æƒ…å ±
            è«–æ–‡æƒ…å ±: {title} - {authors} - {url}
            """
        )
        
        # Runnable APIã‚’ä½¿ç”¨
        analysis_chain = analysis_prompt | llm | StrOutputParser()
        analysis = analysis_chain.invoke({
            "title": title,
            "authors": authors,
            "abstract": abstract,
            "url": paper_url
        })
        
        return analysis
    
    except Exception as e:
        print(f"bioRxivè«–æ–‡åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return f"bioRxivè«–æ–‡åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# ===== ãƒªã‚µãƒ¼ãƒæ©Ÿèƒ½ =====

def academic_research(topic: str) -> str:
    """å­¦è¡“ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã™ã‚‹ç·åˆæ©Ÿèƒ½ï¼ˆArXivã¨bioRxivã‚’å«ã‚€ï¼‰"""
    try:
        print(f"\n=== '{topic}'ã«é–¢ã™ã‚‹å­¦è¡“ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ ===")
        
        # ãƒ†ãƒ¼ãƒã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keyword_prompt = PromptTemplate(
            input_variables=["topic"],
            template="""
            ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã«é–¢ã™ã‚‹åŠ¹æœçš„ãªå­¦è¡“æ¤œç´¢ã®ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å¼•ç”¨ç¬¦ã§å›²ã¾ã‚ŒãŸå½¢ã§æä¾›ã—ã¦ãã ã•ã„ã€‚è‹±èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            
            ç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}
            """
        )
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keyword_chain = keyword_prompt | llm | StrOutputParser()
        keywords_text = keyword_chain.invoke({"topic": topic}).strip()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆå¼•ç”¨ç¬¦ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’å–å¾—ï¼‰
        keywords = re.findall(r'"([^"]*)"', keywords_text)
        if not keywords:
            keywords = [topic]  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒˆãƒ”ãƒƒã‚¯ã‚’ãã®ã¾ã¾ä½¿ç”¨
        
        print(f"æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        
        # ArXivæ¤œç´¢ã¨bioRxivæ¤œç´¢ã‚’å®Ÿè¡Œ
        arxiv_results = []
        biorxiv_results = []
        all_papers = []
        
        arxiv_available = check_arxiv_available()
        
        # æœ€åˆã®Nå€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨ã—ã¦è² è·è»½æ¸›
        for keyword in keywords[:config.keyword_limit]:  
            # ArXivæ¤œç´¢
            if arxiv_available:
                result = optimized_arxiv_search(keyword, max_papers=config.arxiv_research_max_papers)
                if result["status"] == "success":
                    arxiv_results.extend(result["papers"])
                    
            # bioRxivæ¤œç´¢ï¼ˆè‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨ï¼‰
            biorxiv_result = optimized_biorxiv_search(keyword, max_papers=config.biorxiv_research_max_papers)
            if biorxiv_result["status"] == "success":
                biorxiv_results.extend(biorxiv_result["papers"])
                
            time.sleep(2)  # APIåˆ¶é™ã«é…æ…®
        
        # ã™ã¹ã¦ã®è«–æ–‡ã‚’çµåˆ
        all_papers = arxiv_results + biorxiv_results
        
        # é‡è¤‡è«–æ–‡ã‚’é™¤å»
        unique_papers = {}
        for paper in all_papers:
            if paper["url"] not in unique_papers:
                unique_papers[paper["url"]] = paper
        
        all_papers = list(unique_papers.values())
        
        # æ¤œç´¢çµæœã®è¡¨ç¤º
        print(f"\n=== å­¦è¡“æ¤œç´¢çµæœ: åˆè¨ˆ{len(all_papers)}ä»¶ (ArXiv: {len(arxiv_results)}ä»¶, bioRxiv: {len(biorxiv_results)}ä»¶) ===")
        for i, paper in enumerate(all_papers):
            source = paper.get("source", "arxiv")  # ã‚½ãƒ¼ã‚¹ã‚’å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯arxiv
            print(f"{i+1}. [{source}] {paper['title']} - {paper['url']}")
        
        # è«–æ–‡å†…å®¹ã®è¦ç´„
        for paper in all_papers:
            print(f"è«–æ–‡ã‚’è¦ç´„ä¸­: {paper['title']}")
            paper["detailed_summary"] = summarize_paper(paper["content"])
        
        # Webæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆGoogleæ¤œç´¢ãŒä½¿ç”¨å¯èƒ½ãªå ´åˆï¼‰
        web_search_results = ""
        if google_search_available:
            print("Googleæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
            web_search_results = web_search(topic)
            print("Googleæ¤œç´¢å®Œäº†")
        
        # è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—å½¢å¼ã«æ•´å½¢
        all_papers_data = format_papers_data(all_papers)
        
        # Webæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—å½¢å¼ã«æ•´å½¢
        web_search_data = ""
        if web_search_results:
            web_search_data = "## Webæ¤œç´¢ãƒ‡ãƒ¼ã‚¿:\n" + web_search_results
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
        integration_prompt = PromptTemplate(
            input_variables=["topic", "papers_data", "web_search_data", "has_papers", "has_web"],
            template="""
            ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã«é–¢ã™ã‚‹åŒ…æ‹¬çš„ãªå­¦è¡“ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:
            
            ç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}
            
            {papers_data}
            
            {web_search_data}
            
            ä»¥ä¸‹ã®å½¢å¼ã§å­¦è¡“ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:
            
            # {topic}ã«é–¢ã™ã‚‹å­¦è¡“ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆ
            
            ## æ¦‚è¦
            [ãƒ†ãƒ¼ãƒã®æ¦‚è¦ã¨é‡è¦æ€§]
            
            ## ç ”ç©¶ã®ç¾çŠ¶
            [ç¾åœ¨ã®ç ”ç©¶çŠ¶æ³ã¨ä¸»è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ]
            
            ## æœ€æ–°ã®ç ”ç©¶å‹•å‘
            [æœ€è¿‘ã®è«–æ–‡ã‚„æƒ…å ±æºã‹ã‚‰æ˜ã‚‰ã‹ã«ãªã£ãŸæ–°ã—ã„ç™ºè¦‹ã‚„æ–¹æ³•è«–]
            
            ## é‡è¦ãªç™ºè¦‹ã¨ä¸»è¦è«–æ–‡
            [åˆ†é‡ã«ãŠã‘ã‚‹é‡è¦ãªç™ºè¦‹ã¨ãã®æ ¹æ‹ ã¨ãªã‚‹è«–æ–‡ã‚„æƒ…å ±æº]
            
            ## æœªè§£æ±ºã®å•é¡Œã¨å°†æ¥ã®ç ”ç©¶æ–¹å‘
            [ç¾åœ¨ã®ç ”ç©¶ã®é™ç•Œã¨ä»Šå¾Œã®æ–¹å‘æ€§]
            
            ## å‚è€ƒæ–‡çŒ®
            ã“ã“ã§ã¯ã€ä¸Šè¨˜ã§ä½¿ç”¨ã—ãŸæƒ…å ±æºï¼ˆArXivè«–æ–‡ã€bioRxivè«–æ–‡ã€Webæ¤œç´¢çµæœï¼‰ã‚’ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§æä¾›ã—ã¦ãã ã•ã„ã€‚
            å„å‚è€ƒæ–‡çŒ®ã«ã¯ã€ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€URLã¨ã‚½ãƒ¼ã‚¹ï¼ˆArXivã‹bioRxivï¼‰ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
            ä¾‹: [1] è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ« - è‘—è€…å - URL (ArXiv)
            
            æ³¨æ„: å‚è€ƒæ–‡çŒ®ã¯å®Ÿéš›ã«ä¸Šè¨˜ã§æä¾›ã•ã‚ŒãŸæƒ…å ±ã«åŸºã¥ã„ã¦ãã ã•ã„ã€‚æ¶ç©ºã®å‚è€ƒæ–‡çŒ®ã¯ä½œæˆã—ãªã„ã§ãã ã•ã„ã€‚
            """
        )
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        integration_chain = integration_prompt | llm | StrOutputParser()
        final_report = integration_chain.invoke({
            "topic": topic,
            "papers_data": all_papers_data,
            "web_search_data": web_search_data,
            "has_papers": len(all_papers) > 0,
            "has_web": bool(web_search_results)
        })
        
        # å‚è€ƒæ–‡çŒ®ã‚’å¼·èª¿è¡¨ç¤º
        highlighted_report = highlight_references(final_report)
        
        return highlighted_report
        
    except Exception as e:
        print(f"å­¦è¡“ãƒªã‚µãƒ¼ãƒã‚¨ãƒ©ãƒ¼: {e}")
        return f"å­¦è¡“ãƒªã‚µãƒ¼ãƒä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def basic_research(topic: str) -> str:
    """LLMã®çŸ¥è­˜ã®ã¿ã‚’ä½¿ç”¨ã—ãŸåŸºæœ¬ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã™ã‚‹"""
    try:
        # åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        prompt = PromptTemplate(
            input_variables=["topic"],
            template="""
            ã‚ãªãŸã¯ç ”ç©¶ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦è©³ã—ãèª¿æŸ»ã™ã‚‹ãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ã‚ãªãŸã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã¦è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            ãƒˆãƒ”ãƒƒã‚¯: {topic}
            
            ãƒ¬ãƒãƒ¼ãƒˆã¯ä»¥ä¸‹ã®æ§‹é€ ã§ä½œæˆã—ã¦ãã ã•ã„:
            # {topic}ã«é–¢ã™ã‚‹åŸºæœ¬ãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆ
            
            ## æ¦‚è¦
            [ãƒ†ãƒ¼ãƒã®æ¦‚è¦ã¨é‡è¦æ€§]
            
            ## ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ
            1. [ãƒã‚¤ãƒ³ãƒˆ1]
            2. [ãƒã‚¤ãƒ³ãƒˆ2]
            3. [ãƒã‚¤ãƒ³ãƒˆ3]
            
            ## è©³ç´°åˆ†æ
            [è©³ç´°ãªèª¬æ˜ã¨è€ƒå¯Ÿ]
            
            ## çµè«–ã¨å®Ÿè·µçš„å¿œç”¨
            [çµè«–ã¨å®Ÿéš›ã®å¿œç”¨æ–¹æ³•]
            
            æ³¨æ„: ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯LLMã®çŸ¥è­˜ã«åŸºã¥ã„ã¦ãŠã‚Šã€å¤–éƒ¨æ¤œç´¢ã¯ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ã€‚
            """
        )
        
        # Runnable APIã‚’ä½¿ç”¨
        chain = prompt | llm | StrOutputParser()
        response = chain.invoke({"topic": topic})
        return response
        
    except Exception as e:
        print(f"åŸºæœ¬ãƒªã‚µãƒ¼ãƒã‚¨ãƒ©ãƒ¼: {e}")
        return f"ãƒªã‚µãƒ¼ãƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def web_research(topic: str) -> str:
    """Webæ¤œç´¢ã‚’åˆ©ç”¨ã—ãŸãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã™ã‚‹"""
    try:
        print(f"\nWebæ¤œç´¢ã‚’æ´»ç”¨ã—ãŸ '{topic}' ã«é–¢ã™ã‚‹ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹")
        
        if not google_search_available:
            print("Googleæ¤œç´¢ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬ãƒªã‚µãƒ¼ãƒã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
            return basic_research(topic)
        
        # Webæ¤œç´¢å®Ÿè¡Œ
        search_results = web_search(topic)
        
        # æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        research_prompt = PromptTemplate(
            input_variables=["topic", "search_results"],
            template="""
            ä»¥ä¸‹ã®Webæ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã€ç ”ç©¶ãƒ†ãƒ¼ãƒã«é–¢ã™ã‚‹è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:
            
            ç ”ç©¶ãƒ†ãƒ¼ãƒ: {topic}
            
            æ¤œç´¢çµæœ:
            {search_results}
            
            ä»¥ä¸‹ã®å½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:
            
            # {topic}ã«é–¢ã™ã‚‹Webæ¤œç´¢ãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆ
            
            ## æ¦‚è¦
            [ãƒ†ãƒ¼ãƒã®æ¦‚è¦ã¨é‡è¦æ€§]
            
            ## ä¸»è¦ãªæƒ…å ±ã¨ç™ºè¦‹äº‹é …
            [æ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸä¸»è¦ãªæƒ…å ±ã‚’ã¾ã¨ã‚ã‚‹]
            
            ## è©³ç´°åˆ†æ
            [æƒ…å ±ã®è©³ç´°ãªåˆ†æã¨è€ƒå¯Ÿ]
            
            ## çµè«–ã¨å¿œç”¨
            [çµè«–ã¨å®Ÿè·µçš„ãªå¿œç”¨å¯èƒ½æ€§]
            
            ## æƒ…å ±æº
            [æ¤œç´¢çµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±æº]
            """
        )
        
        research_chain = research_prompt | llm | StrOutputParser()
        report = research_chain.invoke({
            "topic": topic,
            "search_results": search_results
        })
        
        return report
        
    except Exception as e:
        print(f"Webæ¤œç´¢ãƒªã‚µãƒ¼ãƒã‚¨ãƒ©ãƒ¼: {e}")
        return f"Webæ¤œç´¢å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ===== Discord Boté–¢æ•° =====

@bot.event
async def on_ready():
    """Botã®èµ·å‹•æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹"""
    print("----------------------------------------")
    print(f'ArXiv & bioRxiv Research Bot Logged in as {bot.user}')
    print("----------------------------------------")

@bot.event
async def on_message(message):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹"""
    # è‡ªåˆ†è‡ªèº«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ç„¡è¦–
    if message.author == bot.user:
        return

    # Botã¸ã®ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‹ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ
    if bot.user.mentioned_in(message) or isinstance(message.channel, discord.DMChannel):
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
        content = message.content.replace(f'<@!{bot.user.id}>', '').replace(f'<@{bot.user.id}>', '').strip()
        
        # ãƒ˜ãƒ«ãƒ—ã‚³ãƒãƒ³ãƒ‰
        if content.lower() == '!help':
            help_text = """
            **ArXiv & bioRxiv Research Bot - ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§**
            
            ãƒœãƒƒãƒˆã®åå‰ã¯`Bot`ã¨ã—ã¾ã™ã€‚
            `@Bot basic: <ãƒ†ãƒ¼ãƒ>` - LLMã®çŸ¥è­˜ã®ã¿ã‚’ä½¿ç”¨ã—ãŸåŸºæœ¬ãƒªã‚µãƒ¼ãƒ
            `@Bot web: <ãƒ†ãƒ¼ãƒ>` - Webæ¤œç´¢ã‚’å«ã‚€ãƒªã‚µãƒ¼ãƒ
            `@Bot academic: <ãƒ†ãƒ¼ãƒ>` - ArXivã¨bioRxivè«–æ–‡ã‚’å«ã‚€å­¦è¡“ãƒªã‚µãƒ¼ãƒ
            `@Bot arxiv: <ãƒ†ãƒ¼ãƒ>` - ArXivè«–æ–‡ã®ã¿ã‚’æ¤œç´¢
            `@Bot biorxiv: <ãƒ†ãƒ¼ãƒ>` - bioRxivè«–æ–‡ã®ã¿ã‚’æ¤œç´¢
            `@Bot paper: <ArXiv ID>` - ArXivè«–æ–‡ã®è©³ç´°åˆ†æ
            `@Bot biopaper: <DOI>` - bioRxivè«–æ–‡ã®è©³ç´°åˆ†æ
            `@Bot save: <ã‚³ãƒãƒ³ãƒ‰>` - çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            `@Bot !help` - ã“ã®ãƒ˜ãƒ«ãƒ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            
            **ä¾‹:**
            `@Bot academic: å¼·åŒ–å­¦ç¿’ã®æœ€æ–°å‹•å‘`
            `@Bot paper: 2501.01433`
            `@Bot biopaper: 10.1101/2022.01.01.123456`
            `@Bot biorxiv: CRISPR`
            `@Bot save: academic: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ`
            """
            await message.channel.send(help_text)
            return

        # ãƒªã‚µãƒ¼ãƒã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
        save_as_file = False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å‡¦ç†
        if content.startswith('save:'):
            save_as_file = True
            content = content[5:].strip()
            
        # ã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
        if content.startswith('basic:'):
            topic = content[6:].strip()
            await message.add_reaction('ğŸ”')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"ã€Œ{topic}ã€ã®åŸºæœ¬ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, basic_research, topic)
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"basic_research_{topic[:20]}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")
                
        elif content.startswith('web:'):
            topic = content[4:].strip()
            await message.add_reaction('ğŸŒ')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"ã€Œ{topic}ã€ã®Webæ¤œç´¢ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            # å®šæœŸçš„ãªé€²æ—å ±å‘Šã®ãŸã‚ã®é–¢æ•°
            async def report_progress():
                progress_messages = [
                    "ğŸ” Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...",
                    "ğŸ“Š æ¤œç´¢çµæœã‚’åˆ†æä¸­...",
                    "âœï¸ ãƒªã‚µãƒ¼ãƒãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."
                ]
                
                for i, msg in enumerate(progress_messages):
                    await asyncio.sleep(20)  # 20ç§’ã”ã¨ã«é€²æ—å ±å‘Š
                    try:
                        await safe_send(message.channel, msg)
                    except:
                        pass  # æ¥ç¶šã‚¨ãƒ©ãƒ¼ãªã©ã‚’ç„¡è¦–
            
            # é€²æ—å ±å‘Šã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
            progress_task = asyncio.create_task(report_progress())
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, web_research, topic)
                
                # é€²æ—å ±å‘Šã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                progress_task.cancel()
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"web_research_{topic[:20]}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")
                
        elif content.startswith('academic:'):
            topic = content[9:].strip()
            await message.add_reaction('ğŸ“š')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"ã€Œ{topic}ã€ã«é–¢ã™ã‚‹å­¦è¡“ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§ãŠå¾…ã¡ãã ã•ã„...")
            
            # å®šæœŸçš„ãªé€²æ—å ±å‘Šã®ãŸã‚ã®é–¢æ•°
            async def report_progress():
                progress_messages = [
                    "ğŸ” ArXivã¨bioRxivè«–æ–‡ã‚’æ¤œç´¢ä¸­...",
                    "ğŸ“„ è«–æ–‡æƒ…å ±ã‚’è§£æä¸­...",
                    "ğŸŒ é–¢é€£æƒ…å ±ã‚’åé›†ä¸­...",
                    "âœï¸ ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."
                ]
                
                for i, msg in enumerate(progress_messages):
                    await asyncio.sleep(30)  # 30ç§’ã”ã¨
                    try:
                        await safe_send(message.channel, msg)
                    except:
                        pass  # æ¥ç¶šã‚¨ãƒ©ãƒ¼ãªã©ã‚’ç„¡è¦–
            
            # é€²æ—å ±å‘Šã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
            progress_task = asyncio.create_task(report_progress())
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, academic_research, topic)
                
                # é€²æ—å ±å‘Šã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                progress_task.cancel()
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"academic_research_{topic[:20]}.md")
                else:
                    # é•·ã„çµæœã‚’åˆ†å‰²ã—ã¦é€ä¿¡
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
                tb_str = ''.join(traceback.format_exception(type(e), e, e.__traceback__))
                print(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼:\n{tb_str}")
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")
            
        elif content.startswith('arxiv:'):
            topic = content[6:].strip()
            await message.add_reaction('ğŸ“œ')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"ã€Œ{topic}ã€ã«é–¢ã™ã‚‹ArXivæ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, lambda: format_papers_data(optimized_arxiv_search(topic)["papers"]))
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"arxiv_search_{topic[:20]}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")

        elif content.startswith('biorxiv:'):
            topic = content[8:].strip()
            await message.add_reaction('ğŸ§¬')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"ã€Œ{topic}ã€ã«é–¢ã™ã‚‹bioRxivæ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, lambda: format_papers_data(optimized_biorxiv_search(topic)["papers"]))
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"biorxiv_search_{topic[:20]}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")
                
        elif content.startswith('paper:'):
            paper_id = content[6:].strip()
            await message.add_reaction('ğŸ“„')
            
            # è«–æ–‡IDã‚’æ•´å½¢
            if not paper_id.startswith('http'):
                paper_id = f"https://arxiv.org/abs/{paper_id}"
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"è«–æ–‡ã€Œ{paper_id}ã€ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, analyze_arxiv_paper, paper_id)
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"paper_analysis_{paper_id.split('/')[-1]}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")
        
        elif content.startswith('biopaper:'):
            doi = content[9:].strip()
            await message.add_reaction('ğŸ§¬')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"bioRxivè«–æ–‡ã€Œ{doi}ã€ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, analyze_biorxiv_paper, doi)
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"biorxiv_analysis_{doi.replace('/', '_')}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")
                
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å­¦è¡“ãƒªã‚µãƒ¼ãƒ
            topic = content
            await message.add_reaction('ğŸ”')
            
            # é€²æ—å ±å‘Š
            await safe_send(message.channel, f"ã€Œ{topic}ã€ã«é–¢ã™ã‚‹å­¦è¡“ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§ãŠå¾…ã¡ãã ã•ã„...")
            
            # å®šæœŸçš„ãªé€²æ—å ±å‘Šã®ãŸã‚ã®é–¢æ•°
            async def report_progress():
                progress_messages = [
                    "ğŸ” ArXivã¨bioRxivè«–æ–‡ã‚’æ¤œç´¢ä¸­...",
                    "ğŸ“„ è«–æ–‡æƒ…å ±ã‚’è§£æä¸­...",
                    "ğŸŒ é–¢é€£æƒ…å ±ã‚’åé›†ä¸­...",
                    "âœï¸ ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."
                ]
                
                for i, msg in enumerate(progress_messages):
                    await asyncio.sleep(30)  # 30ç§’ã”ã¨ã«é€²æ—å ±å‘Š
                    try:
                        await safe_send(message.channel, msg)
                    except:
                        pass  # æ¥ç¶šã‚¨ãƒ©ãƒ¼ãªã©ã‚’ç„¡è¦–
            
            # é€²æ—å ±å‘Šã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
            progress_task = asyncio.create_task(report_progress())
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            try:
                result = await loop.run_in_executor(executor, academic_research, topic)
                
                # é€²æ—å ±å‘Šã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                progress_task.cancel()
                
                if save_as_file:
                    await save_response_as_file(message.channel, result, f"research_{topic[:20]}.md")
                else:
                    await split_and_send_messages(message.channel, result)
                
                await message.add_reaction('âœ…')
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)[:200]}..." if len(str(e)) > 200 else f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                await safe_send(message.channel, f"âŒ {error_msg}")

# Botã®èµ·å‹•
if __name__ == "__main__":
    bot.run(DISCORD_BOT_TOKEN)
